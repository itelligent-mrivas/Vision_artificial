{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c088083c",
   "metadata": {},
   "source": [
    "# Detección de elementos\n",
    "En este ejemplo vamos a usa el modelo [YOLO](https://arxiv.org/pdf/1612.08242.pdf), gracias al cual podremos detectar los elementos que aparecen dentro de una fotgrafia o video.\n",
    "\n",
    "La identificación que obtendremos con YOLO consite en una etiqueta indicando que es lo que se ha detectado y las posiciones en la que se enuentra este objeto.\n",
    "\n",
    "La implementación que usaresmos será la proporcionado por la biblioteca OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770d869",
   "metadata": {},
   "source": [
    "## Carga del modelos\n",
    "A continuación realizamos la carga del modelo preentrenado par la ejecución de YOLO. \n",
    "\n",
    "Se le debe indicar el fichero de clases (que será lo que se podrá detectar), los pesos preentenados y el fichero de configuración del modelo.\n",
    "\n",
    "El fichero de clases y configuración se encuentran en el repositorio, pero el fichero de pesos ocupa demasiado peso para subirlo al resposirorio, por lo que se puede descargar desde el siguiente link https://pjreddie.com/media/files/yolov3.weights , el fichero descargado es necesario copiarlo en la carpeta de recursos_yolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99c6ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "_fichero_clases = './recursos_yolo/yolov3.txt'\n",
    "_fichero_pesos = './recursos_yolo/yolov3.weights'\n",
    "_fichero_configuracion = 'recursos_yolo/yolov3.cfg' \n",
    "\n",
    "# Lectura de las clases\n",
    "classes = None\n",
    "with open(_fichero_clases, 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "# Carga del modelo\n",
    "net = cv2.dnn.readNet(_fichero_pesos, _fichero_configuracion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6dc2fb",
   "metadata": {},
   "source": [
    "## Carga de la imagen\n",
    "\n",
    "Realizamos la carga de la imagen que queremso procesar, asi como su conversión a matriz y su preprocesmainto: escalado, normalizado de colores, etc. para que funcione de manera correcta el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1578bae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.94864    0.97608    0.69776    ... 0.69384    0.63896\n",
      "    0.96824   ]\n",
      "   [0.9604     0.96824    0.70952    ... 0.68208003 0.63896\n",
      "    0.97216   ]\n",
      "   [0.96432    0.98       0.7252     ... 0.686      0.61544\n",
      "    0.98392004]\n",
      "   ...\n",
      "   [0.78008    0.81144    0.8624     ... 0.29008    0.28224\n",
      "    0.28224   ]\n",
      "   [0.79968    0.8232     0.8624     ... 0.30576    0.294\n",
      "    0.29008   ]\n",
      "   [0.79968    0.79968    0.78400004 ... 0.294      0.29792\n",
      "    0.30184   ]]\n",
      "\n",
      "  [[0.91728    0.94472003 0.65856004 ... 0.6664     0.60760003\n",
      "    0.93688   ]\n",
      "   [0.92904    0.93688    0.67424    ... 0.65464    0.60760003\n",
      "    0.9408    ]\n",
      "   [0.93296003 0.94864    0.68992    ... 0.65856004 0.58408\n",
      "    0.95256   ]\n",
      "   ...\n",
      "   [0.69776    0.73304003 0.79184    ... 0.25872    0.25088\n",
      "    0.24696   ]\n",
      "   [0.71736    0.74088    0.78008    ... 0.27048    0.26264\n",
      "    0.25872   ]\n",
      "   [0.71736    0.71736    0.70168    ... 0.26264    0.26656002\n",
      "    0.27048   ]]\n",
      "\n",
      "  [[0.86632    0.89376    0.59584    ... 0.60368    0.55664\n",
      "    0.88592   ]\n",
      "   [0.87808    0.88592    0.61544    ... 0.59192    0.55664\n",
      "    0.88984   ]\n",
      "   [0.882      0.89768    0.63112    ... 0.59584    0.53312004\n",
      "    0.9016    ]\n",
      "   ...\n",
      "   [0.62328    0.65856004 0.70952    ... 0.2156     0.20776\n",
      "    0.20384   ]\n",
      "   [0.64288    0.6664     0.69776    ... 0.21952    0.21168\n",
      "    0.20776   ]\n",
      "   [0.64288    0.64288    0.6272     ... 0.21168    0.2156\n",
      "    0.21952   ]]]]\n"
     ]
    }
   ],
   "source": [
    "_path_imagen = './datasets/test_4.png' \n",
    "\n",
    "image = cv2.imread(_path_imagen)\n",
    "\n",
    "Width = image.shape[1]\n",
    "Height = image.shape[0]\n",
    "scale = 0.00392\n",
    "\n",
    "# Convierte la imagen en una matriz normalizada \n",
    "blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
    "\n",
    "print(blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925d4ba",
   "metadata": {},
   "source": [
    "## Pasamos la imagen por el modelo\n",
    "Una vez que se tiene la imagen convertida en una matriz ya sería posible pasar esta por el modelo.\n",
    "\n",
    "Como resultado de pasar la imagen por el modelo se obtendrán las capas de salida. Que en caso de yolo V3 son multiples capas, como se puede ver a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20d07a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.03809278, 0.0491493 , 0.3619975 , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.04417673, 0.03444803, 0.28926834, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.04399581, 0.03623512, 0.83822274, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.96125   , 0.9481896 , 0.339692  , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.9618722 , 0.96369207, 0.25847152, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.96612704, 0.9625111 , 0.80074924, ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), array([[0.02116642, 0.02072995, 0.05489355, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.01466543, 0.01755857, 0.4054357 , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.01990598, 0.01895546, 0.07683185, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.9726346 , 0.97212344, 0.05717999, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.9799213 , 0.97567517, 0.23019186, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.976271  , 0.9815804 , 0.07943288, ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), array([[0.00906792, 0.00804046, 0.01476534, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.0093918 , 0.010565  , 0.01844392, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.00930176, 0.00779418, 0.20685746, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.99035   , 0.9892503 , 0.01478072, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.98965687, 0.9870074 , 0.0172013 , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.9873622 , 0.99042106, 0.13216464, ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "net.setInput(blob)\n",
    "\n",
    "\n",
    "def obtener_capas_salida(net):\n",
    "    \n",
    "    layer_names = net.getLayerNames()\n",
    "    \n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "# ejecutamos el procesamiento del modelo\n",
    "outs = net.forward(obtener_capas_salida(net))\n",
    "\n",
    "print (outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27836bfd",
   "metadata": {},
   "source": [
    "Recorremos los objetos detectados. Para cada uno ellos obtenemos su valor de cofianza y eliminamos aquellos que no suepren el umbral fijado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a07fa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 63, 0, 0, 56, 56, 56, 39]\n"
     ]
    }
   ],
   "source": [
    "_umbral = 0.8\n",
    "\n",
    "\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > _umbral:\n",
    "            center_x = int(detection[0] * Width)\n",
    "            center_y = int(detection[1] * Height)\n",
    "            w = int(detection[2] * Width)\n",
    "            h = int(detection[3] * Height)\n",
    "            x = center_x - w / 2\n",
    "            y = center_y - h / 2\n",
    "            class_ids.append(class_id)\n",
    "            confidences.append(float(confidence))\n",
    "            boxes.append([x, y, w, h])\n",
    "            \n",
    "print (class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0907366",
   "metadata": {},
   "source": [
    "## Pintado de las cajas de los objetos\n",
    "\n",
    "Una vez qe si tiene los objetos que se han detectado podemos pintar la caja que contenga el objeto y ponerle la etiqueta detectada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e76c10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "#Función para pintar al caja\n",
    "def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "\n",
    "    label = str(classes[class_id])\n",
    "    color = COLORS[class_id]\n",
    "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
    "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "i = 0\n",
    "for box in boxes:\n",
    "    x = box[0]\n",
    "    y = box[1]\n",
    "    w = box[2]\n",
    "    h = box[3]\n",
    "    \n",
    "    draw_bounding_box(image, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "   \n",
    "cv2.imshow(\"object detection\", image)\n",
    "cv2.waitKey()    \n",
    "cv2.imwrite(\"./salidas/object-detection.jpg\", image)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44459ee6",
   "metadata": {},
   "source": [
    "## Refinado de las cajas\n",
    "Procesamos las cajas, para evitar las duplicidades y quedarnos con las máximas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11e4577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_nms_threshold = 0.8\n",
    "            \n",
    "# non-max suppression\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, _umbral, _nms_threshold)\n",
    "\n",
    "for i in indices:\n",
    "    i = i[0]\n",
    "    box = boxes[i]\n",
    "    x = box[0]\n",
    "    y = box[1]\n",
    "    w = box[2]\n",
    "    h = box[3]\n",
    "    \n",
    "    draw_bounding_box(image, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
    "\n",
    " \n",
    "cv2.imshow(\"object detection2\", image)\n",
    "cv2.waitKey()\n",
    "cv2.imwrite(\"./salidas/object-detection2.jpg\", image)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
